{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Final referente a disciplina de Data Science \n",
    "### Aluno: Alves, Carlos F.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Este trabalho faz a analise exploratória dos resultados obtidos do trabalho de mestrado sobre caracterização estatística da transição da convecção rasa para profunda ao longo do experimento GoAmazon2014/15.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "Em relação a trabalhos anteriores, o trabalho de mestrado, propôs um critério para identificação da convecção profunda contornando o limite experimental para detecção de vapor dágua e aproveitando a resolução do GOES-13 além da instrumentação disponível ao longo do período."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT FILES\n",
    "\n",
    "The previous work was accomplished using Matlab, so I did a matlab script to export:\n",
    "- [arm_vars.csv]   : Variable names of Time Series\n",
    "- [arm_values.csv] : ARM DOE Time Series that was resampled by 5min and set NAN values to ones that dont have a good quality check flag provided by ARM \n",
    "- [arm_time.csv]   : date_time of each TS values in Local Time Units\n",
    "- [convective_dates.csv] : convective events selected by previous work methodology\n",
    "\n",
    "Original Data Downloaded from: http://www.archive.arm.gov/discovery/#v/results/s/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD : [arm_vars.csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_vars = []\n",
    "with open('./data/arm_vars.csv', 'r', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        arm_vars.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Parse Units from arm_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_vars_units = []\n",
    "arm_vars_names = []\n",
    "\n",
    "for idx,var in enumerate(arm_vars):    \n",
    "    arm_vars_units.append(re.search(r'\\[(.*?)\\]',arm_vars[idx]).group(1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a hardcoded variable Names To plot Legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_vars_long_names = ['Temperature',\n",
    " 'Relative_Umidity',\n",
    " 'Wind_Speed',\n",
    " 'Rain_Intensity_ORG_815_DA',\n",
    " 'Pressure',\n",
    " 'Wind_Dir',\n",
    " 'Rain_Intensity_PWD22',\n",
    " 'PWV_LOS',\n",
    " 'LWP_LOS',\n",
    " '_',\n",
    " 'Radiative_Flux',\n",
    " 'Percent_opaque_TSI',\n",
    " 'Percent_thin_TSI',\n",
    " 'Surface_Soil_heat_flux',\n",
    " 'Wetness',\n",
    " 'Soil_moisture',\n",
    " 'Rain_Intensity_Disdrometer',\n",
    " 'Median_volume_diameter_Disdrometer',\n",
    " 'PWV_MWR',\n",
    " 'PWV_MWR_23.835_30.0_GHz',\n",
    " 'LWP_MWR',\n",
    " 'LWP_MWR_23.835_30.0_GHz',\n",
    " 'LCL_MWR',\n",
    " 'LFC_MWR',\n",
    " 'CAPE_MWR',\n",
    " 'CloudBaseHeight_MWR',\n",
    " 'Upwelling broadband_hemispheric_irradiance_MFRSR',\n",
    " 'LWP_MWRRET',\n",
    " 'PWV_MWRRET',\n",
    " 'Rain_Intensity_Tipping Bucket',\n",
    " 'Rad_flux_downwelling_shortwave_SKYRAD',\n",
    " 'Rad_flux_downwelling_longwave_SKYRAD',\n",
    " 'Rad_flux_upwelling_shortwave_GNDRAD',\n",
    " 'Rad_flux_upwelling_longwave_GNDRAD',\n",
    " 'Surface_Energy_Balance_SEBS']\n",
    "\n",
    "arm_vars_names = ['T',\n",
    " 'RH',\n",
    " 'Wind_Speed',\n",
    " 'RI_ORG_815_DA',\n",
    " 'P',\n",
    " 'Wind_Dir',\n",
    " 'RI_PWD22',\n",
    " 'PWV_LOS',\n",
    " 'LWP_LOS',\n",
    " '_',\n",
    " 'Radiative_Flux',\n",
    " 'pc_opaque_TSI',\n",
    " 'pc_thin_TSI',\n",
    " 'Surface_Soil_Heat',\n",
    " 'Wetness',\n",
    " 'Soil_moisture',\n",
    " 'RI_Disdrometer',\n",
    " 'Median_D_Disdrometer',\n",
    " 'PWV_MWR',\n",
    " 'PWV_MWR_23_30_GHz',\n",
    " 'LWP_MWR',\n",
    " 'LWP_MWR_23_30_GHz',\n",
    " 'LCL_MWR',\n",
    " 'LFC_MWR',\n",
    " 'CAPE_MWR',\n",
    " 'CBH_MWR',\n",
    " 'Upwelling_IRD_MFRSR',\n",
    " 'LWP_MWRRET',\n",
    " 'PWV_MWRRET',\n",
    " 'RI_Tipping_Bucket',\n",
    " 'Rad_down_shortwave_SKYRAD',\n",
    " 'Rad_down_longwave_SKYRAD',\n",
    " 'Rad_up_shortwave_GNDRAD',\n",
    " 'Rad_up_longwave_GNDRAD',\n",
    " 'Surface_Energy_Balance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map long and variables names with units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 'degC',\n",
       " 'RH': '%',\n",
       " 'Wind_Speed': 'm/s',\n",
       " 'RI_ORG_815_DA': 'mm/h',\n",
       " 'P': 'kPa',\n",
       " 'Wind_Dir': 'º',\n",
       " 'RI_PWD22': 'mm/h',\n",
       " 'PWV_LOS': 'mm',\n",
       " 'LWP_LOS': 'cm',\n",
       " '_': '',\n",
       " 'Radiative_Flux': 'unitless',\n",
       " 'pc_opaque_TSI': '%',\n",
       " 'pc_thin_TSI': '%',\n",
       " 'Surface_Soil_Heat': 'W/m^2',\n",
       " 'Wetness': 'V',\n",
       " 'Soil_moisture': '%',\n",
       " 'RI_Disdrometer': 'mm/h',\n",
       " 'Median_D_Disdrometer': 'mm',\n",
       " 'PWV_MWR': 'cm',\n",
       " 'PWV_MWR_23_30_GHz': 'cm',\n",
       " 'LWP_MWR': 'mm',\n",
       " 'LWP_MWR_23_30_GHz': 'mm',\n",
       " 'LCL_MWR': 'm',\n",
       " 'LFC_MWR': 'm',\n",
       " 'CAPE_MWR': 'J/Kg',\n",
       " 'CBH_MWR': 'm',\n",
       " 'Upwelling_IRD_MFRSR': 'W/m^2',\n",
       " 'LWP_MWRRET': 'g/m^2 ',\n",
       " 'PWV_MWRRET': 'cm',\n",
       " 'RI_Tipping_Bucket': 'mm/h',\n",
       " 'Rad_down_shortwave_SKYRAD': 'W/m^2',\n",
       " 'Rad_down_longwave_SKYRAD': 'W/m^2',\n",
       " 'Rad_up_shortwave_GNDRAD': 'W/m^2',\n",
       " 'Rad_up_longwave_GNDRAD': 'W/m^2',\n",
       " 'Surface_Energy_Balance': 'W/m^2'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_var_units      = dict(zip(arm_vars_names, arm_vars_units))\n",
    "dict_var_short_long = dict(zip(arm_vars_names, arm_vars_long_names))\n",
    "dict_var_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD Dataframe from 'arm_values.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T', 'RH', 'Wind_Speed', 'RI_ORG_815_DA', 'P', 'Wind_Dir', 'RI_PWD22',\n",
       "       'PWV_LOS', 'LWP_LOS', '_', 'Radiative_Flux', 'pc_opaque_TSI',\n",
       "       'pc_thin_TSI', 'Surface_Soil_Heat', 'Wetness', 'Soil_moisture',\n",
       "       'RI_Disdrometer', 'Median_D_Disdrometer', 'PWV_MWR',\n",
       "       'PWV_MWR_23_30_GHz', 'LWP_MWR', 'LWP_MWR_23_30_GHz', 'LCL_MWR',\n",
       "       'LFC_MWR', 'CAPE_MWR', 'CBH_MWR', 'Upwelling_IRD_MFRSR', 'LWP_MWRRET',\n",
       "       'PWV_MWRRET', 'RI_Tipping_Bucket', 'Rad_down_shortwave_SKYRAD',\n",
       "       'Rad_down_longwave_SKYRAD', 'Rad_up_shortwave_GNDRAD',\n",
       "       'Rad_up_longwave_GNDRAD', 'Surface_Energy_Balance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valores =  pd.read_csv('./data/arm_values.csv',names=arm_vars_names)\n",
    "\n",
    "df_valores.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD : [arm_time.csv] \n",
    "\n",
    "output : df_valores\n",
    "\n",
    "Parte Time   :'31-Dec-2013 20:05:06'\n",
    "Using format : \"%d-%b-%Y %H:%M:%S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_time_str = []\n",
    "with open('./data/arm_time.csv', 'r', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        arm_time_str.append(row[0])\n",
    "\n",
    "# PARSE DATE TIME and set INDEX        \n",
    "df_valores.insert(loc=0, column='time_str', value=arm_time_str)\n",
    "df_valores.insert(loc=0, column='dt_datetime', value=datetime.datetime.today())\n",
    "df_valores['dt_datetime'] = datetime.datetime.today()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31-Dec-2013 20:05:06'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valores['time_str'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(df_valores.shape[0]):\n",
    "    df_valores.loc[i,'dt_datetime'] = datetime.datetime.strptime(df_valores['time_str'][i], \"%d-%b-%Y %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores.set_index(df_valores[\"dt_datetime\"],inplace=True)\n",
    "df_valores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores = df_valores.drop('time_str', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage\n",
    "\n",
    "Here we will investivestigate about Data:\n",
    "- Avaiability\n",
    "- Quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullvalueAbs = df_valores.isnull().sum().values\n",
    "nullvaluespc = (df_valores.isnull().sum().values/df_valores.shape[0])*100\n",
    "\n",
    "y_pos = np.arange(len(arm_vars_names))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.barh(y_pos, nullvaluespc[1:], color = 'blue')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(arm_vars_names)\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_xlabel('%')\n",
    "plt.suptitle('Missing Data')\n",
    "plt.gcf().subplots_adjust(left=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all TS with null values > 50 ->df_valores_1\n",
    "\n",
    "OUT: df_valores_1\n",
    "\n",
    "Its possible to see that a loot of variables dont have any data, so we will work only with TimeSeries that have less then 50% of missing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores.columns[nullvaluespc > 50]\n",
    "\n",
    "df_valores_1 = df_valores.copy()\n",
    "df_valores_1 = df_valores_1.drop(df_valores_1.columns[nullvaluespc > 50],axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "for j in np.arange(1,df_valores_1.shape[1]):\n",
    "    plt.subplot((df_valores_1.shape[1]-1),1,j)\n",
    "    plt.plot(df_valores_1[df_valores_1.columns[j]].values,label=df_valores_1.columns[j])\n",
    "    plt.ylabel(\" (\" + dict_var_units[df_valores_1.columns[j]] + \")\")\n",
    "    plt.legend(loc = 'upper right')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Availability in same time interval\n",
    "\n",
    "To make this plot we will just set a different number \"j\" for each valid data, soter in df_window and plot the availability during the field campaing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window = df_valores_1.copy()\n",
    "\n",
    "idxs = np.arange(0,df_valores_1.shape[0])\n",
    "\n",
    "for j in np.arange(1,df_valores_1.shape[1]):\n",
    "    idxs_notnan = idxs[pd.notnull(df_valores_1.iloc[:,j])]\n",
    "    df_window.iloc[idxs_notnan,j] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.yticks(np.arange(2,df_window.shape[1]), df_window.columns[1:])\n",
    "for j in np.arange(1,df_window.shape[1]):\n",
    "    plt.plot(df_window[df_window.columns[j]].values)\n",
    "plt.title('Data Availability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT : TimeDate range that all TS has Valid Data --> df_valores_2\n",
    "\n",
    "OUT: df_valores_2\n",
    "\n",
    "Its possible to see that exist just a range during the campaing that all TS has data, so we will use a ANDING process in mask_not_null and get the first and the last time that ALL TS have data. After that we will slice df_valore_1 and create df_valores_2 corresponding to that period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_not_null = pd.notnull(df_valores_1.iloc[:,1]).values\n",
    "    \n",
    "for j in np.arange(2,df_valores_1.shape[1]):\n",
    "    mask_not_null = mask_not_null & pd.notnull(df_valores_1.iloc[:,j]).values\n",
    "\n",
    "commom_valid_idx = idxs[mask_not_null]    \n",
    "first_valid_idx = commom_valid_idx[0]    \n",
    "last_valid_idx = commom_valid_idx[-1]    \n",
    "\n",
    "np.arange(0,first_valid_idx)\n",
    "\n",
    "start = df_valores_1.iloc[first_valid_idx,0]\n",
    "end =  df_valores_1.iloc[last_valid_idx,0]\n",
    "\n",
    "df_valores_2 = df_valores_1[(df_valores_1.dt_datetime > start) &  (df_valores_1.dt_datetime < end)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Quality : Visual inspect each TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_vars = df_valores_2.columns[np.hstack((np.arange(1,8),10))]\n",
    "termo_vars = df_valores_2.columns[np.hstack((np.arange(8,10),np.arange(11,19)))]\n",
    "rad_vars   = df_valores_2.columns[19:]\n",
    "\n",
    "rain_intensity_vars = meteo_vars[[3,6,7]]\n",
    "meteo_vars_exclude_rain = set(meteo_vars)-set(rain_intensity_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "meteo_vars_exclude_rain = set(meteo_vars)-set(meteo_vars[[3,6,7]])\n",
    "for idx,var in enumerate(meteo_vars_exclude_rain):\n",
    "    plt.subplot(len(meteo_vars_exclude_rain),1,idx+1)\n",
    "    plt.plot(df_valores_2[var].values,label=var)\n",
    "    plt.ylabel(dict_var_units[var])\n",
    "    plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SET NAN to Temperatura < 10, impossible in Central Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores_2.loc[df_valores_2['T'] < 10] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Offtset of  RH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores_2.columns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(df_valores_2.shape[0]):\n",
    "    if df_valores_2.iloc[i,2]  > 100:\n",
    "        offset = df_valores_2.iloc[i,2] - 100\n",
    "        df_valores_2.iloc[i,2] = df_valores_2.iloc[i,2] - offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAIN intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "for idx,var in enumerate(rain_intensity_vars ):\n",
    "    plt.subplot(len(rain_intensity_vars),1,idx+1)\n",
    "    plt.plot(df_valores_2[var].values,label=var)\n",
    "    plt.ylabel(dict_var_units[var])\n",
    "    plt.legend(loc = 'upper right')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set NAN to Rain Intensity PWD  > 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores_2.loc[df_valores_2['RI_PWD22'] > 200] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "for idx,var in enumerate(rain_intensity_vars ):\n",
    "    plt.subplot(len(rain_intensity_vars),1,idx+1)\n",
    "    plt.plot(df_valores_2[var].values,label=var)\n",
    "    plt.ylabel(dict_var_units[var])\n",
    "    plt.legend(loc = 'upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop PWD, too small values  --> df_valores_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores_2.columns[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores_3 = df_valores_2.copy()\n",
    "df_valores_3 = df_valores_3.drop(df_valores_3.columns[7],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiometric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAD vars\n",
    "rad_vars   = df_valores_3.columns[18:]\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "for idx,var in enumerate(rad_vars):\n",
    "    plt.subplot(len(rad_vars),1,idx+1)\n",
    "    plt.plot(df_valores_3[var].values,label=var)\n",
    "    plt.ylabel(dict_var_units[var])\n",
    "    plt.legend(loc = 'upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transform \n",
    "\n",
    "OUT: df_15min\n",
    "\n",
    "Due to slow processing to perform Data Exploration the data will be resampled to 15min and will be recorded in df_15min data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min = pd.DataFrame()\n",
    "for idx,c in enumerate(df_valores_3.columns[1:]):\n",
    "    df_15min[c] = df_valores_3[c].resample('15min').mean()\n",
    "df_15min.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_vars = ['T', 'P', 'RH', 'Wind_Speed','Wind_Dir','CBH_MWR']\n",
    "rain_vars = ['RI_ORG_815_DA','RI_Disdrometer'] \n",
    "termo_vars = ['PWV_LOS', 'LWP_LOS', 'PWV_MWR','PWV_MWR_23_30_GHz', 'LWP_MWR', 'LWP_MWR_23_30_GHz', 'LCL_MWR', 'LFC_MWR', 'CAPE_MWR']\n",
    "rad_vars = ['Upwelling_IRD_MFRSR','Rad_down_shortwave_SKYRAD','Rad_down_longwave_SKYRAD','Rad_up_shortwave_GNDRAD','Rad_up_longwave_GNDRAD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE Correlations \n",
    "\n",
    "To inspect about linear and instant correlations we used here scatter matrix plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = scatter_matrix(df_15min[rad_vars], alpha=0.2,figsize=(12, 10) ,diagonal='kde')\n",
    "plt.suptitle('Radiometrics Distribution')\n",
    "#Change label rotation\n",
    "[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-0.3,0.5) for s in sm.reshape(-1)]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot indicates that in general radiometric variables presents some linear correlation. Its important to inpect by other type of correlations: nonlinear and not instantaneos.\n",
    "This plot resulted in a fited distribution of each variable, that lays in matrix diagonal. Its possible to see a tunned long wave radiation emittion and a narrow peak of incoming long wave, that indicate the cloud scattering to the ground\n",
    "Further analysis of shortwave distribution is needed, maybe this distribution indicates a scale-free process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = scatter_matrix(df_15min[meteo_vars], alpha=0.2,figsize=(12, 10), diagonal='kde')\n",
    "plt.suptitle('Meteo Vars Distribution')\n",
    "#Change label rotation\n",
    "[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-0.3,0.5) for s in sm.reshape(-1)]\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = scatter_matrix(df_15min[termo_vars], alpha=0.2, figsize=(12, 10),diagonal='kde')\n",
    "plt.suptitle('Termo Vars Distribution')\n",
    "#Change label rotation\n",
    "[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-0.3,0.5) for s in sm.reshape(-1)]\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Convective Days\n",
    "\n",
    "[convective_dates.csv] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convectives =  pd.read_csv('./data/convective_dates.csv',names=['event_time_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convectives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_list = df_convectives.index.tolist()\n",
    "\n",
    "for i in np.arange(0,len(as_list)):\n",
    "    tmp = as_list[i]\n",
    "    as_list[i] = tmp[0:-1]\n",
    "\n",
    "as_list[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convectives.insert(loc=0, column='time_str', value=as_list)\n",
    "df_convectives = df_convectives.drop('event_time_str',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_convectives.shape[0])\n",
    "df_convectives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convectives.insert(loc=0, column='dt_datetime', value=datetime.datetime.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1,df_convectives.shape[0]):\n",
    "    df_convectives.iloc[i,0] = datetime.datetime.strptime(df_convectives['time_str'][i], '%d-%b-%Y %H:%M:%S')\n",
    "df_convectives.set_index(df_convectives[\"dt_datetime\"],inplace=True)    \n",
    "df_convectives.drop('time_str',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = resampled Datetime of first convective day\n",
    "\n",
    "end   = resampled Datetime of last convective day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = df_15min.index[0]\n",
    "end = df_15min.index[-1]\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convectives_1 = df_convectives[ (df_convectives['dt_datetime'] >= start ) & (df_convectives['dt_datetime'] <= end )].copy()\n",
    "df_convectives_1['dt_datetime'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convectives_1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Matrix for each TS\n",
    "\n",
    "| var = T  |D1 |D2 |D3 |... |D326|\n",
    "|--------------------------------|   \n",
    "|t1        |   |   |   |    |    |\n",
    "|t2        |   |   |   |    |    |\n",
    "|t3        |   |   |   |    |    |\n",
    "|...       |   |   |   |    |    |\n",
    "|t23       |   |   |   |    |    |\n",
    "\n",
    "\n",
    "This kind of structure is used to explore each convective day for each TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15min.insert(loc=0, column='dt_datetime', value=df_15min.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 132 # df_convectives\n",
    "i = 2   # df_convectives_1\n",
    "dfSelected = df_15min[ (df_15min['dt_datetime'].dt.day == df_convectives_1.dt_datetime.iloc[i].day) & (df_15min['dt_datetime'].dt.month == df_convectives_1.dt_datetime.iloc[i].month) & (df_15min['dt_datetime'].dt.year == df_convectives_1.dt_datetime.iloc[i].year)].copy()\n",
    "print(len(dfSelected['T'][:].values))\n",
    "time_length = len(dfSelected['T'][:].values)\n",
    "dfSelected['T'][:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create nMatrix to store TS to each convective day \n",
    "Each numpy array has the exact name of df_15min.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din_var = {}\n",
    "for idx in np.arange(1,len(df_15min.columns)):\n",
    "    globals()[df_15min.columns[idx]] = np.zeros((time_length,df_convectives_1.shape[0],),dtype=np.float64)\n",
    "    globals()[str('df_') + df_15min.columns[idx]] = pd.DataFrame()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# assing all times of event = 2 in var = 1\n",
    "print(df_15min.columns[1])\n",
    "vars()['T'][:,2] = dfSelected['T'][:].values\n",
    "print(vars()['T'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ievent in np.arange(df_convectives_1.shape[0]):\n",
    "    dfSelected = df_15min[ (df_15min['dt_datetime'].dt.day == df_convectives_1.dt_datetime.iloc[ievent].day) & (df_15min['dt_datetime'].dt.month == df_convectives_1.dt_datetime.iloc[ievent].month) & (df_15min['dt_datetime'].dt.year == df_convectives_1.dt_datetime.iloc[ievent].year)].copy()\n",
    "    non_zero_list = []\n",
    "    for c in np.arange(1,len(df_15min.columns)):\n",
    "        values_size = dfSelected[dfSelected.columns[c]].values.size\n",
    "        non_zero = np.count_nonzero(dfSelected[dfSelected.columns[c]].values)\n",
    "        \n",
    "        if values_size > 0 :\n",
    "            #print(dfSelected[dfSelected.columns[c]].values)\n",
    "            vars()[df_15min.columns[c]][:,ievent] = dfSelected[dfSelected.columns[c]][:].values\n",
    "            non_zero_list.append(non_zero)\n",
    "        \n",
    "    print(\"Event : \" + str(df_convectives.shape[0]-ievent) + \"  (\" + str(non_zero_list) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "teste = np.transpose(LFC_MWR)\n",
    "teste[10,:] == LFC_MWR[:,10]\n",
    "series = np.asmatrix(teste)\n",
    "\n",
    "from dtaidistance import dtw\n",
    "\n",
    "ds = dtw.distance_matrix_fast(series)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of this error we must investigate further to decice what to do with NAN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance import clustering\n",
    "\n",
    "model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
    "model2 = clustering.HierarchicalTree(model1)\n",
    "model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n",
    "\n",
    "cluster_idx = model3.fit(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Missing Value by event for each TS and All TS with more than 20 Missing Value per Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(1,15):\n",
    "\n",
    "    valores= np.arange(0,vars()[df_15min.columns[idx]].shape[1])\n",
    "    idx_v = np.arange(0,vars()[df_15min.columns[idx]].shape[1])\n",
    "\n",
    "    for ievent in np.arange(0,vars()[df_15min.columns[idx]].shape[1]):\n",
    "        valores[ievent] = np.isnan(vars()[df_15min.columns[idx]][:,ievent]).sum()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(valores)\n",
    "    plt.ylabel(df_15min.columns[idx] + 'Missing Data')\n",
    "    plt.xlabel('events')\n",
    "    plt.title(df_15min.columns[idx] + ' Missing Data Count ')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in np.arange(0,len(valores[valores > 10])):\n",
    "        plt.subplot(len(valores[valores > 10]),1,i+1)\n",
    "        plt.plot(vars()[df_15min.columns[idx]][:,i],'.')\n",
    "        plt.suptitle(df_15min.columns[idx] + ' : Events with More than 10 Missing ' )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(15,len(df_15min.columns)):\n",
    "\n",
    "    valores= np.arange(0,vars()[df_15min.columns[idx]].shape[1])\n",
    "    idx_v = np.arange(0,vars()[df_15min.columns[idx]].shape[1])\n",
    "\n",
    "    for ievent in np.arange(0,vars()[df_15min.columns[idx]].shape[1]):\n",
    "        valores[ievent] = np.isnan(vars()[df_15min.columns[idx]][:,ievent]).sum()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(valores)\n",
    "    plt.ylabel(df_15min.columns[idx] + 'Missing Data')\n",
    "    plt.xlabel('events')\n",
    "    plt.title(df_15min.columns[idx] + ' Missing Data Count ')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in np.arange(0,len(valores[valores > 10])):\n",
    "        plt.subplot(len(valores[valores > 10]),1,i+1)\n",
    "        plt.plot(vars()[df_15min.columns[idx]][:,i],'.')\n",
    "        plt.suptitle(df_15min.columns[idx] + ' : Events with More than 10 Missing ' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate using akima, repeat last one and fill all other nans with row mean  (same local time)\n",
    "\n",
    "And finally due to a persistent existence of null values, row mean calculation to substitute in the remaining nan, of each column for the same row.\n",
    "Remember that each column is a deep convective event and each line is a tn of this day due to periodicity of time series, so each line has the same value of Local Time. The mean valued of each row represents the men value of the variable of an especific Time of day during almost 2 years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(1,len(df_15min.columns)):\n",
    "    np_i = vars()[df_15min.columns[idx]]\n",
    "    vars()[str('df_') + df_15min.columns[idx]] = pd.DataFrame(data=np_i[:],copy=True) \n",
    "    df_i = vars()[str('df_') + df_15min.columns[idx]]\n",
    "    df_i = df_i.interpolate(method='akima',axis=0)\n",
    "    df_i = df_i.fillna(method='pad', limit=1)\n",
    "    vars()[str('df_') + df_15min.columns[idx]] = df_i\n",
    "    m = df_i.mean(axis=1)\n",
    "    for i, col in enumerate(df_i):\n",
    "        df_i.iloc[:, i] = df_i.iloc[:, i].fillna(m)    \n",
    "    print(str(df_i.isna().sum().sum()) + '  :  ' + df_15min.columns[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(1,len(df_15min.columns)):\n",
    "    vect_i  = vars()[df_15min.columns[idx]]\n",
    "    df_i    = vars()[str('df_') + df_15min.columns[idx]]\n",
    "    \n",
    "    valores = np.arange(0,vect_i.shape[1])\n",
    "    idx_v   = np.arange(0,vect_i.shape[1])\n",
    "\n",
    "    for ievent in np.arange(0,vect_i.shape[1]):\n",
    "        valores[ievent] = np.isnan(vect_i[:,ievent]).sum()\n",
    "\n",
    "    print(str(df_i.isna().sum().sum()) + '  :  ' + df_15min.columns[idx])\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in np.arange(0,len(valores[valores > 10])):\n",
    "        plt.subplot(len(valores[valores > 10]),1,i+1)\n",
    "        plt.plot(vect_i[:,i],'*-')\n",
    "        plt.plot(df_i.iloc[:,i].values,'*-')\n",
    "        plt.suptitle(df_15min.columns[idx] + ' : Events with More than 10 Missing ' )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.arange(1,len(df_15min.columns)):\n",
    "    vect_i  = vars()[df_15min.columns[idx]]\n",
    "    df_i    = vars()[str('df_') + df_15min.columns[idx]]\n",
    "    series  = np.transpose(df_i.as_matrix())\n",
    "    \n",
    "    ds = dtw.distance_matrix_fast(series)\n",
    "    model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
    "    model2 = clustering.HierarchicalTree(model1)\n",
    "    model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n",
    "    cluster_idx = model3.fit(series)    \n",
    "    model2.plot\n",
    "    model3.plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "model2.plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "for idx in np.arange(1,len(df_15min.columns)):\n",
    "    df_i    = vars()[str('df_') + df_15min.columns[idx]]\n",
    "    z = linkage(df_i.iloc[:,0:181],'ward')\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    dendrogram(z)\n",
    "    plt.title('Hierarchical Cluster ' + df_15min.columns[idx])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
